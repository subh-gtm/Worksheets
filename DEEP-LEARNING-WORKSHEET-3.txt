
            DEEP LEARNING – WORKSHEET 3 


Q1 to Q8 are MCQs with only one correct answer. Choose the correct option. 


1. Which of the following is true about model capacity (where model capacity means the ability of neural 
   network to approximate complex functions)? 
   A) As dropout ratio increases, model capacity increases 
   B) As number of hidden layers increase, model capacity increases 
   C) As learning rate increases, model capacity increases 
   D) None of the above 

Answer1: B

2. Batch Normalization is helpful because? 
   A) It is a very efficient backpropagation technique 
   B) It returns back the normalized mean and standard deviation of weights 
   C) It normalizes (changes) all the input before sending it to the next layer  
   D) None of the above 

Answer2: C

3. What if we use a learning rate that’s too large? 
   A) Network will not converge        B) Network will converge 
   C) either A or B                    D) None of the above  

Answer3: A

4. What are the factors to select the depth of neural network? 
   i) Type of neural network (e.g. MLP, CNN etc.) 
   ii) Input data 
   iii) Computation power, i.e. Hardware capabilities and software capabilities 
   iv) Learning Rate 
   v) The output function to map 
   A) 1, 2, 4, 5               B) 2, 3, 4, 5 
   C) 1, 3, 4, 5               D) All of these  

Answer4: D

5. Suppose you have inputs as x, y, and z with values -2, 5, and -4 respectively. You have a neuron ‘q’ and 
   neuron ‘f’ with functions: 
   q = x + y 
   f = q * z 
   Graphical representation of the functions is as follows: 
 
   What is the gradient of F with respect to x, y, and z? (use chain rule of derivatives to find the solution) 
   A) (3, -4, -4)              B) (-3, 4, 4) 
   C) (-4, -4, 3)              D) (4, 4, 3) 

Answer5:C

6. Which of the following statement is the best description of early stopping? 
   A) Train the network until a local minimum in the error function is reached                 
   B) Simulate the network on a test dataset after every epoch of training. Stop training when the generalization  
      error starts to increase 
   C) Add a momentum term to the weight update in the Generalized Delta Rule, so that training converges more  
     quickly 
   D) None of the above 
 
Answer6: B
 
7. Which  gradient  descent  technique  is  more  advantageous  when  the  data  is  too  big  to  handle  in  RAM 
   simultaneously? 
   A) Mini Batch Gradient Descent      B) Stochastic Gradient Descent 
   C) Full Batch Gradient Descent      D) either A or B 

Answer7: B

8. Consider the scenario. The problem you are trying to solve has a small amount of data. Fortunately, you have a 
   pre-trained neural network that was trained on a similar problem. Which of the following methodologies would 
   you choose to make use of this pre-trained network?  
   A) Freeze all the layers except the last, re-train the last layer 
   B) Assess on every layer how the model performs and only select a few of them 
   C) Fine tune the last couple of layers only 
   D) Re-train the model for the new dataset 

Answer8: A

Q9 and Q10 are MCQs with one or more correct answers. Choose all the correct options. 


9. Which of the following neural network training challenge can be solved using batch normalization? 
   A) Overfitting         B) Training is too slow 
   C) Restrict activations to become too high or low 
   D) None of these 

Answer9: B,C

10. For a binary classification problem, which of the following activations may be used in output layer? 
    A) ReLU                   B) sigmoid 
    C) softmax                D) Leaky ReLU 

Answer10: A,D

Q11 to Q15 are subjective answer type question. Answer them briefly. 

11. What will happen if we do not use activation function in artificial neural networks? 
Answer11:Neural network without the activation functions, In this case, every neuron will only be performing a linear transformation
         on the inputs using the weights and biases. Although linear transformations make the neural network simpler, but this network 
         would be less powerful and will not be able to learn the complex patterns from the data. so we use a non linear transformation
         to the inputs of the neuron and this non-linearity in the network is introduced by an activation function.

12. How does forward propagation and backpropagation work in deep learning? 
Answer12: Forward propagation :Propagating the computations of all neurons within all layers moving from left to right. This starts with the feeding
          of our feature into the input layer, and ends with the final prediction generated by the output layer. Forward pass computations occur during
          training in order to evaluate the objective/loss function under the current network parameter settings in each iteration, 
          as well as during inference (prediction after training) when applied to new/unseen data.

          Backward propagation :This is a step executed during training in order to compute the objective/loss function gradient with respect to the network’s
          parameters for updating them during a single iteration of some form of gradient descent. It is named as such because, when viewing a neural 
          network as a computation graph, it starts by computing objective/loss function derivatives at the output layer, and propagates them back towards
          the input layer (effectively, this is the chain rule from Calculus in action) in order to compute derivatives for, and make updates to, all 
          parameters in all layers.

13. Explain briefly the following variant of Gradient Descent: Stochastic, Batch, and Mini-batch? 
Answer13:Stochastic gradient descent, often abbreviated SGD, is a variation of the gradient descent algorithm that calculates the error and
         updates the model for each example in the training dataset.

         Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, 
         but only updates the model after all training examples have been evaluated.

        Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that
        are used to calculate model error and update model coefficients. Implementations may choose to sum the gradient over the mini-batch which further
        reduces the variance of the gradient. Mini-batch gradient descent seeks to find a balance between the robustness of stochastic gradient descent
        and the efficiency of batch gradient descent. It is the most common implementation of gradient descent used in the field of deep learning.

14. What are the main benefits of Mini-batch Gradient Descent? 
Answer14: The model update frequency is higher than batch gradient descent which allows for a more robust convergence, avoiding local minima.
          The batched updates provide a computationally more efficient process than stochastic gradient descent.
          The batching allows both the efficiency of not having all training data in memory and algorithm implementations.


15. What is transfer learning? 
Answer15: Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.
          It is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language
          processing tasks given the vast compute and time resources required to develop neural network models on these problems and from the huge 
          jumps in skill that they provide on related problems.
 
 